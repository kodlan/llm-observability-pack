# vLLM Model Configuration
# --------------------------

# HuggingFace model identifier
# Small models suitable for RTX 2070 (8GB VRAM):
#   - Qwen/Qwen2.5-1.5B-Instruct (recommended)
#   - Qwen/Qwen2.5-0.5B-Instruct
#   - microsoft/phi-2
#   - google/gemma-2-2b-it
#   - TinyLlama/TinyLlama-1.1B-Chat-v1.0
MODEL_NAME=Qwen/Qwen2.5-1.5B-Instruct

# Maximum context length in tokens
# Lower values reduce memory usage
VLLM_MAX_MODEL_LEN=2048

# Fraction of GPU memory to use (0.0 - 1.0)
# Leave headroom for KV cache and system
VLLM_GPU_MEMORY_UTILIZATION=0.8
